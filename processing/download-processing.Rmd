---
title: "[EXPERIMENT NAME] Data Processing (change title to reflect your experiment)"
output: html_notebook

This code tutorial does three things:
  
  1. Downloads experiment data files from AWS
  2. Selects the incomplete files based on file size
  3. Moves the incomplete files to a new folder
  
If you are unable to run this code for any reason, you can also do this manually (without using the code). You will use Cyberduck to download data and sort through incomplete files by looking at file sizes. To do this manually, follow the tutorial here: https://github.com/GoldenbergLab/lab-helper-codes/blob/main/guides/aws/s3-tasks/download.md
---
We use groundhog (https://groundhogr.com/) to manage our packages. Groundhog makes sure that our package versions are consistent with the current date. This way, everyone will be using packages from the same date to prevent any issues related to package versioning in future analyses. 

To use groundhog, please: 
(1) make sure that you have the latest version of R/Rstudio installed 
(2) install RTools
(3) make sure you are in a new R Session

The R Version used to create this file is: `R version 4.3.0 (2023-04-21 ucrt)`. You can change this out with your own version of R if you are using this template to create your own analysis. If you are reproducing another person's analysis, please download the version of R used by the previous person to ensure a proper reproduction of the code.

You can identify your version of R by typing `version` into the R Console

You can switch your R version by pressing `Tools` > `Global Options` > `R Version`

```{r libraries, include = FALSE}

# if you need to install groundhog
#if(!suppressWarnings(require(groundhog))){remotes::install_github('CredibilityLab/groundhog')}

#call groundhog
library("groundhog")


# put all packages in and call groundhog
pkgs <- c(
    "aws.s3",
    "tidyverse"
    )

pkgs_all <- c(pkgs)

# call groundhog. Change out the date for today's date (unless you are running an old analysis)
groundhog.library(pkgs_all, "2023-04-23")

# this line of code runs better in Windows
#groundhog.library(pkgs_all, "2023-04-23",ignore.deps = c('rlang', 'fs' ) )

```


If this line of code does not run, you can alternatively download the data via Cyberduck and manually move the raw data objects into the data folder. Tutorial here: https://github.com/GoldenbergLab/lab-helper-codes/blob/main/guides/aws/s3-tasks/download.md

paste the following code into the R Console to set AWS credentials. For AWS credentials, reach out to Amit or the lab manager.

Sys.setenv("AWS_ACCESS_KEY_ID" = "mykey",
       "AWS_SECRET_ACCESS_KEY" = "mysecretkey",
        "AWS_DEFAULT_REGION" = "us-east-1")


```{r download data and examine file sizes for cutoff point}

## Download raw data objects and put them into the "data" folder

#Provides a list of all of the buckets in the list 
bucketlist("s3://task-data-raw/")

#define your aws-s3 folder
aws_folder ="bills-task-pilot"

#download all the files from that folder to your raw library 
system(paste("aws s3 cp s3://task-data-raw/",aws_folder," ../data/raw --recursive", sep = ""))




```


Filter files by size to figure out which data files are incomplete and should therefore be removed

This part often does not work (especially on Windows). If this is the case, please manually examine the files, decide which ones are incomplete and move the incomplete ones into the "did not finish" folder

```{r aws}



#create a list of the files and their sizes
raw_files = file.info(list.files ("../data/raw", 
                                  full.names =TRUE, 
                                  recursive=FALSE)) %>% 
  arrange(size) %>% 
  mutate(size_kb = size/1000)

ggplot(raw_files,aes(x=size_kb)) +
  geom_histogram(color = "black") 

#define your kb file size criteria for files you wish to remove
size_criteria = 150
shell(paste("find ../data/raw/ -type f -size -150k -exec mv ../data/raw ../data/raw/did_not_finish/+"))

system(paste("mv ../data/raw ../data/raw/did_not_finish/ " ))


path =  "../data/raw" #path to the raw files (of good size)

#get all csv files and combine them
csv_files <- fs::dir_ls(path, regexp = "\\.csv$") 

#map dataframe on to csvs
d = csv_files %>% 
  map_dfr(read_csv)


```


Process the data in the next few chunks. The final output of this data processing should be one long-format csv that contains one trial per line.

```{r}

```

