---
title: "[EXPERIMENT NAME] Data Processing (change title to reflect your experiment)"
output: html_notebook

This code tutorial does two things:
  
  1. Downloads experiment data files from AWS
  2. Selects the incomplete files based on file size
  3. Moves the incomplete files to a new folder
  
If you are unable to run this code for any reason, you can also do this manually. To do this manually, follow the tutorial here: 
---
We use groundhog (https://groundhogr.com/) to manage our packages. Groundhog makes sure that our package versions are consistent with the date that 
We are running the analysis to prevent any issues related to package versioning in future analyses. 

To use groundhog, please: 
(1) make sure that you have the latest version of R/ RStudio installed; 
(2) install RTools

```{r libraries, include = FALSE}

#Run this line if you don't have groundhog installed.
#remotes::install_github('CredibilityLab/groundhog',upgrade = "always")

#call groundhog
if(!suppressWarnings(require(groundhog))){remotes::install_github('CredibilityLab/groundhog');library("groundhog")}


######################## put all packages in and call groundhog
pkgs <- c(
    "aws.s3",
    "tidyverse"
    )

pkgs_all <- c(pkgs)

########################## call groundhog. Change out the date for today's date (unless you are running an old analysis)
groundhog.library(pkgs_all, "2023-04-23")

############# this line of code runs better in Windows
#groundhog.library(pkgs_all, "2023-04-23",ignore.deps = c('rlang', 'fs' ) )

```


```{r download data and examine file sizes for cutoff point}

## Download raw data objects and put them into the "data" folder

## If this line of code does not run, you can alternatively download the data via Cyberduck/ manually move the raw data objects into the data folder. Tutorial here: https://github.com/GoldenbergLab/lab-helper-codes/blob/main/guides/aws/s3-tasks/download.md

################ paste the following code into the R Console to set AWS credentials
# Sys.setenv("AWS_ACCESS_KEY_ID" = "mykey",
#        "AWS_SECRET_ACCESS_KEY" = "mysecretkey",
#        "AWS_DEFAULT_REGION" = "us-east-1")

#Provides a list of all of the buckets in the list 
bucketlist("s3://task-data-raw/")

#define your aws-s3 folder
aws_folder ="bills-task-pilot"

#download all the files from that folder to your raw library 
system(paste("aws s3 cp s3://task-data-raw/",aws_folder," ../data/raw --recursive", sep = ""))




```


```{r}

## filter files by size to figure out which data files are incomplete and should therefore be removed
## this part often does not work. If this is the case, please manually examine the files, decide which ones are incomplete and move the incomplete ones into the "did not finish" folder

#create a list of the files and their sizes
raw_files = file.info(list.files ("../data/raw", 
                                  full.names =TRUE, 
                                  recursive=FALSE)) %>% 
  arrange(size) %>% 
  mutate(size_kb = size/1000)

ggplot(raw_files,aes(x=size_kb)) +
  geom_histogram(color = "black") 

#define your kb file size criteria for files you wish to remove
size_criteria = 150
shell(paste("find ../data/raw/ -type f -size -150k -exec mv ../data/raw ../data/raw/did_not_finish/+"))

system(paste("mv ../data/raw ../data/raw/did_not_finish/ " ))


path =  "../data/raw" #path to the raw files (of good size)

#get all csv files and combine them
csv_files <- fs::dir_ls(path, regexp = "\\.csv$") 

#map dataframe on to csvs
d = csv_files %>% 
  map_dfr(read_csv)


# Process the data in the next few chunks. The final output of this data processing should be one long-format csv that contains one trial per line.

```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

